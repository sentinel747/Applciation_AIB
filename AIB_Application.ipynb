{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install pandas openpyxl\n",
    "%pip install transformers\n",
    "%pip install newspaper3k\n",
    "%pip install google-search-results\n",
    "%pip install googletrans\n",
    "%pip install llama-cpp-python\n",
    "%pip install tqdm\n",
    "%pip install googlesearch-python\n",
    "%pip install seaborn\n",
    "import pandas as pd\n",
    "df_data = pd.read_excel(\"https://github.com/sentinel747/Applciation_AIB/raw/refs/heads/main/DataInputCRMExraction.xlsx\")\n",
    "df_context = pd.read_excel(\"https://github.com/sentinel747/Applciation_AIB/raw/refs/heads/main/ContextInputCategories.xlsx\")\n",
    "# Imposto le opzioni per mostrare tutte le righe e colonne\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Importa tqdm per la barra di avanzamento\n",
    "\n",
    "# Rimozione delle colonne inutili\n",
    "df_data_cleaned = df_data.drop(columns=[\"Contact Name\", \"Contact Job Title\"])\n",
    "\n",
    "# Rimozione di duplicati e valori mancanti\n",
    "df_data_cleaned = df_data_cleaned.drop_duplicates()\n",
    "df_data_cleaned = df_data_cleaned.dropna()\n",
    "\n",
    "# Estrazione del dominio dall'email\n",
    "tqdm.pandas(desc=\"Estrazione domini\")\n",
    "df_data_cleaned['Email Domain'] = df_data_cleaned['Contact E-mail'].progress_apply(lambda email: email.split('@')[-1])\n",
    "\n",
    "# Rimozione dei domini pubblici\n",
    "public_domains = ['gmail.com', 'hotmail.com', 'yahoo.com', 'outlook.com', 'live.com', 'icloud.com', 'unknown.com']\n",
    "df_data_cleaned = df_data_cleaned[~df_data_cleaned['Email Domain'].isin(public_domains)]\n",
    "\n",
    "# Verifica dell'esistenza dei domini\n",
    "def check_domain_exists(domain):\n",
    "    try:\n",
    "        response = requests.get(f\"http://{domain}\", timeout=2)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "tqdm.pandas(desc=\"Verifica dei domini\")\n",
    "df_data_cleaned['Domain Exists'] = df_data_cleaned['Email Domain'].progress_apply(check_domain_exists)\n",
    "df_data_cleaned = df_data_cleaned[df_data_cleaned['Domain Exists']]\n",
    "df_data_cleaned = df_data_cleaned.drop(columns=['Domain Exists'])  # Rimuove la colonna temporanea\n",
    "\n",
    "# Estrazione del nome principale del dominio (ad es., \"savills\" da \"savills.es\" e \"savills.pt\")\n",
    "df_data_cleaned['Main Domain Name'] = df_data_cleaned['Email Domain'].apply(lambda domain: domain.split('.')[0])\n",
    "\n",
    "# Rimozione dei duplicati basati sul nome principale del dominio mantenendo la prima occorrenza del dominio completo\n",
    "df_data_cleaned = df_data_cleaned.drop_duplicates(subset=[\"Main Domain Name\"])\n",
    "\n",
    "# Rimozione della colonna temporanea \"Main Domain Name\"\n",
    "df_data_cleaned = df_data_cleaned.drop(columns=['Main Domain Name'])\n",
    "\n",
    "# Rimozione dei duplicati finali su \"Contact E-mail\" e \"Email Domain\"\n",
    "df_data_cleaned = df_data_cleaned.drop_duplicates(subset=[\"Contact E-mail\"])\n",
    "df_data_cleaned = df_data_cleaned.drop_duplicates(subset=[\"Email Domain\"])\n",
    "\n",
    "# Salvataggio del risultato finale\n",
    "df_data_cleaned.to_excel(r'C:\\Users\\Andrea\\Downloads\\df_data_cleaned.xlsx', index=False)\n",
    "\n",
    "df_data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Funzione per pulire il dataset delle categorie\n",
    "def clean_categories(df):\n",
    "    # Rimuovi duplicati e valori nulli\n",
    "    df_cleaned = df.drop_duplicates().dropna()\n",
    "\n",
    "    # Rimuovi le colonne specificate\n",
    "    df_cleaned = df_cleaned.drop(columns=['Can they buy the solution?', 'Can they influence the buying decision?'])\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# Applica la funzione di pulizia ai dati delle categorie\n",
    "df_context_cleaned = clean_categories(df_context)\n",
    "df_context_cleaned.to_excel(r'C:\\Users\\Andrea\\Downloads\\df_context_cleaned.xlsx', index=False)\n",
    "df_context_cleaned.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup # type: ignore\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Importa tqdm per la barra di avanzamento\n",
    "from transformers import pipeline\n",
    "\n",
    "# Inizializzazione del modello di Hugging Face per la traduzione\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")  # Modello per tradurre in inglese\n",
    "\n",
    "# Funzione per rimuovere duplicati da una lista di stringhe\n",
    "def remove_duplicates(description_parts):\n",
    "    seen = set()\n",
    "    unique_parts = []\n",
    "    for part in description_parts:\n",
    "        if part not in seen:\n",
    "            unique_parts.append(part)\n",
    "            seen.add(part)\n",
    "    return unique_parts\n",
    "\n",
    "# Funzione migliorata per estrarre più informazioni dai meta tag e dai paragrafi <p>\n",
    "def extract_meta_information(soup):\n",
    "    info = {}\n",
    "\n",
    "    # Estrarre meta description\n",
    "    description = soup.find('meta', attrs={'name': 'description'})\n",
    "    if description:\n",
    "        info['meta_description'] = description.get('content', '')\n",
    "\n",
    "    # Estrarre og:description (usato spesso per social e anteprime)\n",
    "    og_description = soup.find('meta', attrs={'property': 'og:description'})\n",
    "    if og_description:\n",
    "        info['og_description'] = og_description.get('content', '')\n",
    "\n",
    "    # Estrarre meta keywords (se disponibili)\n",
    "    keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords:\n",
    "        info['meta_keywords'] = keywords.get('content', '')\n",
    "\n",
    "    # Estrarre og:title (utilizzato per anteprime sui social)\n",
    "    og_title = soup.find('meta', attrs={'property': 'og:title'})\n",
    "    if og_title:\n",
    "        info['og_title'] = og_title.get('content', '')\n",
    "\n",
    "    # Estrazione di più paragrafi per raccogliere maggiori informazioni\n",
    "    paragraphs = soup.find_all('p')\n",
    "    if paragraphs:\n",
    "        # Limitiamo l'estrazione a un massimo di 7 paragrafi, per evitare di estrarre troppo testo\n",
    "        paragraph_text = \" \".join([p.get_text().strip() for p in paragraphs[:7] if p.get_text().strip()])\n",
    "        info['paragraphs'] = paragraph_text\n",
    "\n",
    "    return info\n",
    "\n",
    "# Funzione per estrarre informazioni dal sito web\n",
    "def extract_company_info(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Usa la funzione migliorata per estrarre le informazioni\n",
    "            meta_info = extract_meta_information(soup)\n",
    "\n",
    "            # Combina le informazioni raccolte per creare una descrizione\n",
    "            description_parts = []\n",
    "\n",
    "            # Aggiungi le varie descrizioni disponibili\n",
    "            if 'meta_description' in meta_info:\n",
    "                description_parts.append(meta_info['meta_description'])\n",
    "            if 'og_description' in meta_info:\n",
    "                description_parts.append(meta_info['og_description'])\n",
    "            if 'paragraphs' in meta_info:\n",
    "                description_parts.append(meta_info['paragraphs'])\n",
    "\n",
    "            # Rimuovi duplicati dalle descrizioni\n",
    "            unique_description_parts = remove_duplicates(description_parts)\n",
    "\n",
    "            # Unisci le parti di descrizione\n",
    "            full_description = \" \".join(unique_description_parts)\n",
    "            return full_description[:500]  # Limita a 500 caratteri\n",
    "        else:\n",
    "            return \"Company information not available.\"\n",
    "    except Exception as e:\n",
    "        return \"Company information not available.\"\n",
    "\n",
    "# Funzione per tradurre la descrizione in inglese\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        translation = translator(text, max_length=500)\n",
    "        return translation[0]['translation_text']\n",
    "    except Exception as e:\n",
    "        return text\n",
    "\n",
    "# Funzione per creare il link del sito web a partire dal dominio email\n",
    "def construct_website_url(domain):\n",
    "    return f\"http://{domain}\"\n",
    "\n",
    "# Funzione per aggiungere la colonna \"Description\" nel DataFrame con barra di progresso\n",
    "def add_description_column(df):\n",
    "    descriptions = []\n",
    "    \n",
    "    # Usa tqdm per visualizzare la barra di avanzamento\n",
    "    for domain in tqdm(df['Email Domain'], desc=\"Estrazione informazioni\", unit=\"azienda\"):\n",
    "        website_url = construct_website_url(domain)\n",
    "        web_description = extract_company_info(website_url)\n",
    "        translated_description = translate_to_english(web_description)\n",
    "        descriptions.append(translated_description)\n",
    "\n",
    "    df['Description'] = descriptions  # Aggiungi tutte le descrizioni al DataFrame\n",
    "    return df\n",
    "\n",
    "# Esegui la funzione per aggiungere la colonna 'Description' al DataFrame esistente\n",
    "df_data_with_description = add_description_column(df_data_cleaned)\n",
    "df_data_with_description.to_excel(r'C:\\Users\\Andrea\\Downloads\\df_data_with_company_description.xlsx', index=False)\n",
    "\n",
    "df_data_with_description.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Funzione per eseguire la classificazione basata su TF-IDF e similarità coseno\n",
    "def classify_with_cosine_similarity(df_data, df_context):\n",
    "    # Combina le descrizioni e le note per costruire il vocabolario\n",
    "    all_texts = df_data['Description'].tolist() + df_context['Notes'].tolist()\n",
    "\n",
    "    # Creazione della rappresentazione TF-IDF per le descrizioni e le note\n",
    "    vectorizer = TfidfVectorizer().fit(all_texts)\n",
    "    company_vectors = vectorizer.transform(df_data['Description'])\n",
    "    category_vectors = vectorizer.transform(df_context['Notes'])\n",
    "\n",
    "    best_matches = []\n",
    "    similarities_matrix = []\n",
    "\n",
    "    # Calcola la similarità coseno tra ogni descrizione aziendale e le note delle categorie\n",
    "    for i in tqdm(range(company_vectors.shape[0]), desc=\"Classificazione\", unit=\"azienda\"):\n",
    "        similarities = cosine_similarity(company_vectors[i], category_vectors)\n",
    "        similarities_matrix.append(similarities.flatten())  # Appiattisci la matrice delle similarità\n",
    "        best_match_idx = similarities.argmax()  # Trova l'indice con la similarità massima\n",
    "        best_category = df_context.iloc[best_match_idx]['Player']  # Ottieni la categoria corrispondente\n",
    "        best_matches.append(best_category)\n",
    "\n",
    "    # Assegna la migliore categoria trovata al DataFrame\n",
    "    df_data['Best Category'] = best_matches\n",
    "    return df_data, pd.DataFrame(similarities_matrix)\n",
    "\n",
    "# Funzione per aggiungere la colonna \"Best Category\" e \"Is Target\" nel DataFrame\n",
    "def add_classification_and_target_column(df_data, df_context):\n",
    "    # Applica la classificazione basata su similarità coseno e ottieni la matrice di similarità\n",
    "    df_data, similarities_matrix = classify_with_cosine_similarity(df_data, df_context)\n",
    "\n",
    "    # Creazione della mappa per \"Is Target\"\n",
    "    target_map = df_context.set_index('Player')['Is Target'].to_dict()\n",
    "\n",
    "    # Funzione per verificare se la categoria è un target\n",
    "    def is_target_category(category):\n",
    "        return target_map.get(category, \"No\")  # Restituisce \"No\" se la categoria non è trovata\n",
    "\n",
    "    # Aggiunge la colonna \"Is Target\" in base alla categoria assegnata\n",
    "    df_data['Is Target'] = df_data['Best Category'].apply(lambda category: is_target_category(category))\n",
    "\n",
    "    return df_data, similarities_matrix\n",
    "\n",
    "# Funzione per visualizzare la distribuzione delle categorie (Target vs Non-Target)\n",
    "def plot_target_distribution(df_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.countplot(x='Is Target', data=df_data)\n",
    "    \n",
    "    # Aggiungi le etichette sopra ogni barra\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='baseline', fontsize=12, color='black', xytext=(0, 5),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "    plt.title('Distribuzione delle Aziende tra Target e Non-Target')\n",
    "    plt.show()\n",
    "\n",
    "# Funzione per visualizzare la distribuzione delle categorie (Player)\n",
    "def plot_player_distribution(df_data):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.countplot(y='Best Category', data=df_data, order=df_data['Best Category'].value_counts().index)\n",
    "    \n",
    "    # Aggiungi le etichette per il numero di aziende per ciascuna categoria\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_width())}', (p.get_width(), p.get_y() + p.get_height() / 2),\n",
    "                    ha='center', va='baseline', fontsize=12, color='black', xytext=(0, 5),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "    plt.title('Distribuzione delle Aziende per Categoria (Player)')\n",
    "    plt.show()\n",
    "\n",
    "# Funzione per visualizzare la heatmap delle similarità coseno\n",
    "def plot_similarity_heatmap(similarities_matrix):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarities_matrix, cmap=\"YlGnBu\", xticklabels=False, yticklabels=False)\n",
    "    plt.title(\"Heatmap delle Similarità Coseno tra Aziende e Categorie\")\n",
    "    plt.show()\n",
    "\n",
    "# Funzione per visualizzare un istogramma della distribuzione delle similarità\n",
    "def plot_similarity_histogram(similarities_matrix):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(similarities_matrix.to_numpy().flatten(), bins=20, kde=True)  # Conversione in NumPy array\n",
    "    plt.title(\"Distribuzione delle Similarità Coseno tra Aziende e Categorie\")\n",
    "    plt.xlabel(\"Valori di Similarità Coseno\")\n",
    "    plt.ylabel(\"Frequenza\")\n",
    "    plt.show()\n",
    "\n",
    "# Funzione per visualizzare la percentuale di target\n",
    "def plot_target_percentage(df_data):\n",
    "    target_counts = df_data['Is Target'].value_counts(normalize=True) * 100\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=140, colors=['#ff9999','#66b3ff'])\n",
    "    plt.title(\"Percentuale di Aziende Target vs Non-Target\")\n",
    "    plt.show()\n",
    "\n",
    "# Applica la classificazione e la verifica del target utilizzando la similarità coseno\n",
    "df_data_classified_cosinesimilarity, similarities_matrix_cosinesimilarity = add_classification_and_target_column(df_data_with_description, df_context_cleaned)\n",
    "\n",
    "# Salva il DataFrame aggiornato con le categorie classificate e la colonna Is Target\n",
    "output_path_llm = r'C:\\Users\\Andrea\\Downloads\\data_classified_cosinesimilarity.xlsx'\n",
    "df_data_classified_cosinesimilarity.to_excel(output_path_llm, index=False)\n",
    "\n",
    "# Visualizza i grafici\n",
    "plot_target_distribution(df_data_classified_cosinesimilarity)\n",
    "plot_target_percentage(df_data_classified_cosinesimilarity)  # Aggiunta del grafico per la percentuale di Target\n",
    "plot_player_distribution(df_data_classified_cosinesimilarity)\n",
    "plot_similarity_heatmap(similarities_matrix_cosinesimilarity)\n",
    "plot_similarity_histogram(similarities_matrix_cosinesimilarity)\n",
    "\n",
    "df_data_classified_cosinesimilarity\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
